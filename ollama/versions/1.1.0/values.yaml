# Alternatives: llava, gemmma, mistral, etc.
defaultModel: llama3

workload:
  containers:
    ui:
      name: ollama-ui
      image: ghcr.io/open-webui/open-webui:main
      port: 8080
      resources:
        cpu: 500m
        memory: 1Gi
    api:
      name: ollama
      image: ollama/ollama
      port: 11434
      resources:
        cpu: 6
        memory: 8Gi
      gpu:
        nvidia:
          model: t4
          quantity: 1

volumeset:
  initialCapacity: 10
  autoscaling:
    enabled: false # Set to true to enable autoscaling
    maxCapacity: 100 # Maximum capacity in GiB when autoscaling is enabled
    minFreePercentage: 10 # Minimum free percentage to trigger scaling when autoscaling is enabled
    scalingFactor: 1.2 # Scaling factor to determine how much to scale up when autoscaling is triggered
  performanceClass: general-purpose-ssd
  snapshots:
    retentionDuration: 7d

firewall:
  external: # Change to restrict access to the workload if needed
    inboundAllowCIDR:
        - 0.0.0.0/0
    outboundAllowCIDR:
        - 0.0.0.0/0
internal_access:
  type: same-gvc # options: same-gvc, same-org, workload-list
  workloads:  # Note: can only be used if type is same-gvc or workload-list
    #- //gvc/GVC_NAME/workload/WORKLOAD_NAME
    #- //gvc/GVC_NAME/workload/WORKLOAD_NAME

entrypoint:
  payload: |
    #!/bin/bash
    # Define the model directory
    MODEL_DIR="/root/.ollama/models/manifests/registry.ollama.ai/library/$DEFAULT_MODELS/"
    # Start ollama serve in the background
    /bin/ollama serve &
    # Check if the model directory exists
    if [ ! -d "$MODEL_DIR" ]; then
        echo "Model directory not found. Pulling the $DEFAULT_MODELS model..."
        # Pull the $DEFAULT_MODELS model using the Ollama API
        apt-get update && apt-get install curl -y
        curl http://localhost:11434/api/pull -d '{
            "name": "$DEFAULT_MODELS"
        }'
    else
        echo "Model directory exists. No action required."
    fi
    # Keep the script running
    while true; do sleep 86400; done
